package openai

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"net/http"
)

// Chat message role defined by the OpenAI API.
const (
	ChatMessageRoleSystem    = "system"
	ChatMessageRoleUser      = "user"
	ChatMessageRoleAssistant = "assistant"
	ChatMessageRoleFunction  = "function"
	ChatMessageRoleTool      = "tool"
	ChatMessageRoleDeveloper = "developer"
)

const chatCompletionsSuffix = "/chat/completions"

var (
	ErrChatCompletionInvalidModel       = errors.New("this model is not supported with this method, please use CreateCompletion client method instead") //nolint:lll
	ErrChatCompletionStreamNotSupported = errors.New("streaming is not supported with this method, please use CreateChatCompletionStream")              //nolint:lll
	ErrContentFieldsMisused             = errors.New("can't use both Content and MultiContent properties simultaneously")
)

type Hate struct {
	Filtered bool   `json:"filtered"`
	Severity string `json:"severity,omitempty"`
}
type SelfHarm struct {
	Filtered bool   `json:"filtered"`
	Severity string `json:"severity,omitempty"`
}
type Sexual struct {
	Filtered bool   `json:"filtered"`
	Severity string `json:"severity,omitempty"`
}
type Violence struct {
	Filtered bool   `json:"filtered"`
	Severity string `json:"severity,omitempty"`
}

type JailBreak struct {
	Filtered bool `json:"filtered"`
	Detected bool `json:"detected"`
}

type Profanity struct {
	Filtered bool `json:"filtered"`
	Detected bool `json:"detected"`
}

type ContentFilterResults struct {
	Hate      Hate      `json:"hate,omitempty"`
	SelfHarm  SelfHarm  `json:"self_harm,omitempty"`
	Sexual    Sexual    `json:"sexual,omitempty"`
	Violence  Violence  `json:"violence,omitempty"`
	JailBreak JailBreak `json:"jailbreak,omitempty"`
	Profanity Profanity `json:"profanity,omitempty"`
}

type PromptAnnotation struct {
	PromptIndex          int                  `json:"prompt_index,omitempty"`
	ContentFilterResults ContentFilterResults `json:"content_filter_results,omitempty"`
}

type ImageURLDetail string

const (
	ImageURLDetailHigh ImageURLDetail = "high"
	ImageURLDetailLow  ImageURLDetail = "low"
	ImageURLDetailAuto ImageURLDetail = "auto"
)

type ChatMessageImageURL struct {
	URL    string         `json:"url,omitempty"`
	Detail ImageURLDetail `json:"detail,omitempty"`
}

type ChatMessagePartType string

const (
	ChatMessagePartTypeText     ChatMessagePartType = "text"
	ChatMessagePartTypeImageURL ChatMessagePartType = "image_url"
	ChatMessagePartTypeDocument ChatMessagePartType = "document"
)

// DocumentSource represents the source of a document for Anthropic citations
type DocumentSource struct {
	Type      string `json:"type"`       // "text" or "pdf"
	MediaType string `json:"media_type"` // MIME type like "text/plain"
	Data      string `json:"data"`       // Document content
}

// DocumentCitations controls citation behavior for Anthropic
type DocumentCitations struct {
	Enabled bool `json:"enabled"`
}

type ChatMessagePart struct {
	Type         ChatMessagePartType    `json:"type,omitempty"`
	Text         string                 `json:"text,omitempty"`
	ImageURL     *ChatMessageImageURL   `json:"image_url,omitempty"`
	CacheControl *CacheControl          `json:"cache_control,omitempty"`
	Source       *DocumentSource        `json:"source,omitempty"`       // For document type
	Title        string                 `json:"title,omitempty"`        // For document type
	Citations    *DocumentCitations     `json:"citations,omitempty"`    // For document type
}

type CacheControlType string

const (
	CacheControlTypeEphemeral CacheControlType = "ephemeral"
)

type CacheControl struct {
	Type CacheControlType `json:"type,omitempty"`
}

type ChatCompletionMessage struct {
	Role         string `json:"role"`
	Content      string `json:"content,omitempty"`
	Refusal      string `json:"refusal,omitempty"`
	MultiContent []ChatMessagePart

	// This property isn't in the official documentation, but it's in
	// the documentation for the official library for python:
	// - https://github.com/openai/openai-python/blob/main/chatml.md
	// - https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
	Name string `json:"name,omitempty"`

	// This property is used for the "reasoning" feature supported by deepseek-reasoner
	// which is not in the official documentation.
	// the doc from deepseek:
	// - https://api-docs.deepseek.com/api/create-chat-completion#responses
	ReasoningContent string `json:"reasoning_content,omitempty"`

	FunctionCall *FunctionCall `json:"function_call,omitempty"`

	// For Role=assistant prompts this may be set to the tool calls generated by the model, such as function calls.
	ToolCalls []ToolCall `json:"tool_calls,omitempty"`

	// For Role=tool prompts this should be set to the ID given in the assistant's prior request to call a tool.
	ToolCallID string `json:"tool_call_id,omitempty"`

	// ProviderSpecificFields contains provider-specific response data (e.g., citations from Anthropic via LiteLLM)
	// Example: response.choices[0].message.provider_specific_fields["citations"]
	ProviderSpecificFields map[string]any `json:"provider_specific_fields,omitempty"`
}

// AnthropicCitation represents a citation returned by Anthropic's citation API
// When accessed via LiteLLM, includes additional transformation fields like SupportedText
type AnthropicCitation struct {
	Type          string `json:"type"`            // e.g., "char_location", "page_location", "content_block_location"
	CitedText     string `json:"cited_text"`      // The exact text being cited from source
	DocumentIndex int    `json:"document_index"`  // Index of the document (0-based)
	DocumentTitle string `json:"document_title"`  // Title of the cited document
	FileID        string `json:"file_id,omitempty"` // Optional file identifier

	// For char_location type (plain text documents)
	StartChar int `json:"start_char_index,omitempty"` // Character index where citation starts (0-indexed)
	EndChar   int `json:"end_char_index,omitempty"`   // Character index where citation ends (exclusive)

	// For page_location type (PDF documents)
	StartPage int `json:"start_page_number,omitempty"` // Page number where citation starts (1-indexed)
	EndPage   int `json:"end_page_number,omitempty"`   // Page number where citation ends (exclusive)

	// For content_block_location type (custom content documents)
	StartBlock int `json:"start_block_index,omitempty"` // Block index where citation starts (0-indexed)
	EndBlock   int `json:"end_block_index,omitempty"`   // Block index where citation ends (exclusive)

	// LiteLLM transformation field - the response text that this citation supports
	SupportedText string `json:"supported_text,omitempty"`
}

// GetAnthropicCitations extracts Anthropic citations from provider_specific_fields
// Returns error if citations field is not present or not in expected Anthropic format
func (m *ChatCompletionMessage) GetAnthropicCitations() ([][]AnthropicCitation, error) {
	if m.ProviderSpecificFields == nil {
		return nil, fmt.Errorf("no provider_specific_fields present")
	}

	citations, ok := m.ProviderSpecificFields["citations"]
	if !ok {
		return nil, fmt.Errorf("no citations field in provider_specific_fields")
	}

	// Marshal back to JSON and unmarshal into typed struct
	// This handles the nested array structure: [[[citation, citation], [citation]], ...]
	b, err := json.Marshal(citations)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal citations: %w", err)
	}

	var result [][]AnthropicCitation
	if err := json.Unmarshal(b, &result); err != nil {
		return nil, fmt.Errorf("failed to unmarshal citations as Anthropic format: %w", err)
	}

	return result, nil
}

func (m ChatCompletionMessage) MarshalJSON() ([]byte, error) {
	if m.Content != "" && m.MultiContent != nil {
		return nil, ErrContentFieldsMisused
	}
	if len(m.MultiContent) > 0 {
		msg := struct {
			Role                   string            `json:"role"`
			Content                string            `json:"-"`
			Refusal                string            `json:"refusal,omitempty"`
			MultiContent           []ChatMessagePart `json:"content,omitempty"`
			Name                   string            `json:"name,omitempty"`
			ReasoningContent       string            `json:"reasoning_content,omitempty"`
			FunctionCall           *FunctionCall     `json:"function_call,omitempty"`
			ToolCalls              []ToolCall        `json:"tool_calls,omitempty"`
			ToolCallID             string            `json:"tool_call_id,omitempty"`
			ProviderSpecificFields map[string]any    `json:"provider_specific_fields,omitempty"`
		}(m)
		return json.Marshal(msg)
	}

	msg := struct {
		Role                   string            `json:"role"`
		Content                string            `json:"content,omitempty"`
		Refusal                string            `json:"refusal,omitempty"`
		MultiContent           []ChatMessagePart `json:"-"`
		Name                   string            `json:"name,omitempty"`
		ReasoningContent       string            `json:"reasoning_content,omitempty"`
		FunctionCall           *FunctionCall     `json:"function_call,omitempty"`
		ToolCalls              []ToolCall        `json:"tool_calls,omitempty"`
		ToolCallID             string            `json:"tool_call_id,omitempty"`
		ProviderSpecificFields map[string]any    `json:"provider_specific_fields,omitempty"`
	}(m)
	return json.Marshal(msg)
}

func (m *ChatCompletionMessage) UnmarshalJSON(bs []byte) error {
	msg := struct {
		Role                   string `json:"role"`
		Content                string `json:"content"`
		Refusal                string `json:"refusal,omitempty"`
		MultiContent           []ChatMessagePart
		Name                   string            `json:"name,omitempty"`
		ReasoningContent       string            `json:"reasoning_content,omitempty"`
		FunctionCall           *FunctionCall     `json:"function_call,omitempty"`
		ToolCalls              []ToolCall        `json:"tool_calls,omitempty"`
		ToolCallID             string            `json:"tool_call_id,omitempty"`
		ProviderSpecificFields map[string]any    `json:"provider_specific_fields,omitempty"`
	}{}

	if err := json.Unmarshal(bs, &msg); err == nil {
		*m = ChatCompletionMessage(msg)
		return nil
	}
	multiMsg := struct {
		Role                   string `json:"role"`
		Content                string
		Refusal                string            `json:"refusal,omitempty"`
		MultiContent           []ChatMessagePart `json:"content"`
		Name                   string            `json:"name,omitempty"`
		ReasoningContent       string            `json:"reasoning_content,omitempty"`
		FunctionCall           *FunctionCall     `json:"function_call,omitempty"`
		ToolCalls              []ToolCall        `json:"tool_calls,omitempty"`
		ToolCallID             string            `json:"tool_call_id,omitempty"`
		ProviderSpecificFields map[string]any    `json:"provider_specific_fields,omitempty"`
	}{}
	if err := json.Unmarshal(bs, &multiMsg); err != nil {
		return err
	}
	*m = ChatCompletionMessage(multiMsg)
	return nil
}

type ToolCall struct {
	// Index is not nil only in chat completion chunk object
	Index    *int         `json:"index,omitempty"`
	ID       string       `json:"id,omitempty"`
	Type     ToolType     `json:"type"`
	Function FunctionCall `json:"function"`
}

type FunctionCall struct {
	Name string `json:"name,omitempty"`
	// call function with arguments in JSON format
	Arguments string `json:"arguments,omitempty"`
}

type ChatCompletionResponseFormatType string

const (
	ChatCompletionResponseFormatTypeJSONObject ChatCompletionResponseFormatType = "json_object"
	ChatCompletionResponseFormatTypeJSONSchema ChatCompletionResponseFormatType = "json_schema"
	ChatCompletionResponseFormatTypeText       ChatCompletionResponseFormatType = "text"
)

type ChatCompletionResponseFormat struct {
	Type       ChatCompletionResponseFormatType        `json:"type,omitempty"`
	JSONSchema *ChatCompletionResponseFormatJSONSchema `json:"json_schema,omitempty"`
}

type ChatCompletionResponseFormatJSONSchema struct {
	Name        string         `json:"name"`
	Description string         `json:"description,omitempty"`
	Schema      json.Marshaler `json:"schema"`
	Strict      bool           `json:"strict"`
}

// ChatCompletionRequest represents a request structure for chat completion API.
type ChatCompletionRequest struct {
	Model    string                  `json:"model"`
	Messages []ChatCompletionMessage `json:"messages"`
	// MaxTokens The maximum number of tokens that can be generated in the chat completion.
	// This value can be used to control costs for text generated via API.
	// This value is now deprecated in favor of max_completion_tokens, and is not compatible with o1 series models.
	// refs: https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_tokens
	MaxTokens int `json:"max_tokens,omitempty"`
	// MaxCompletionTokens An upper bound for the number of tokens that can be generated for a completion,
	// including visible output tokens and reasoning tokens https://platform.openai.com/docs/guides/reasoning
	MaxCompletionTokens int                           `json:"max_completion_tokens,omitempty"`
	Temperature         float32                       `json:"temperature,omitempty"`
	TopP                float32                       `json:"top_p,omitempty"`
	N                   int                           `json:"n,omitempty"`
	Stream              bool                          `json:"stream,omitempty"`
	Stop                []string                      `json:"stop,omitempty"`
	PresencePenalty     float32                       `json:"presence_penalty,omitempty"`
	ResponseFormat      *ChatCompletionResponseFormat `json:"response_format,omitempty"`
	Seed                *int                          `json:"seed,omitempty"`
	FrequencyPenalty    float32                       `json:"frequency_penalty,omitempty"`
	// LogitBias is must be a token id string (specified by their token ID in the tokenizer), not a word string.
	// incorrect: `"logit_bias":{"You": 6}`, correct: `"logit_bias":{"1639": 6}`
	// refs: https://platform.openai.com/docs/api-reference/chat/create#chat/create-logit_bias
	LogitBias map[string]int `json:"logit_bias,omitempty"`
	// LogProbs indicates whether to return log probabilities of the output tokens or not.
	// If true, returns the log probabilities of each output token returned in the content of message.
	// This option is currently not available on the gpt-4-vision-preview model.
	LogProbs bool `json:"logprobs,omitempty"`
	// TopLogProbs is an integer between 0 and 5 specifying the number of most likely tokens to return at each
	// token position, each with an associated log probability.
	// logprobs must be set to true if this parameter is used.
	TopLogProbs int    `json:"top_logprobs,omitempty"`
	User        string `json:"user,omitempty"`
	// Deprecated: use Tools instead.
	Functions []FunctionDefinition `json:"functions,omitempty"`
	// Deprecated: use ToolChoice instead.
	FunctionCall any    `json:"function_call,omitempty"`
	Tools        []Tool `json:"tools,omitempty"`
	// This can be either a string or an ToolChoice object.
	ToolChoice any `json:"tool_choice,omitempty"`
	// Options for streaming response. Only set this when you set stream: true.
	StreamOptions *StreamOptions `json:"stream_options,omitempty"`
	// Disable the default behavior of parallel tool calls by setting it: false.
	ParallelToolCalls any `json:"parallel_tool_calls,omitempty"`
	// Store can be set to true to store the output of this completion request for use in distillations and evals.
	// https://platform.openai.com/docs/api-reference/chat/create#chat-create-store
	Store bool `json:"store,omitempty"`
	// Controls effort on reasoning for reasoning models. It can be set to "low", "medium", or "high".
	ReasoningEffort string `json:"reasoning_effort,omitempty"`
	// Metadata to store with the completion.
	Metadata map[string]string `json:"metadata,omitempty"`
	// Configuration for a predicted output.
	Prediction *Prediction `json:"prediction,omitempty"`
	// ChatTemplateKwargs provides a way to add non-standard parameters to the request body.
	// Additional kwargs to pass to the template renderer. Will be accessible by the chat template.
	// Such as think mode for qwen3. "chat_template_kwargs": {"enable_thinking": false}
	// https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes
	ChatTemplateKwargs map[string]any `json:"chat_template_kwargs,omitempty"`
	// ExtraHeaders allows passing custom HTTP headers
	ExtraHeaders http.Header `json:"-"`
}

type StreamOptions struct {
	// If set, an additional chunk will be streamed before the data: [DONE] message.
	// The usage field on this chunk shows the token usage statistics for the entire request,
	// and the choices field will always be an empty array.
	// All other chunks will also include a usage field, but with a null value.
	IncludeUsage bool `json:"include_usage,omitempty"`
}

type ToolType string

const (
	ToolTypeFunction            ToolType = "function"
	ToolTypeTextEditor20241022  ToolType = "text_editor_20241022"
	ToolTypeTextEditor20250124  ToolType = "text_editor_20250124"
	ToolTypeTextEditor20250728  ToolType = "text_editor_20250728"
	ToolTypeMemory20250818      ToolType = "memory_20250818"
)

type Tool struct {
	Type     ToolType            `json:"type"`
	Function *FunctionDefinition `json:"function,omitempty"`
	Name     string              `json:"name,omitempty"` // For Anthropic-defined tools like text_editor_20250124
}

// MarshalJSON customizes JSON marshaling for Tool to properly handle Anthropic-defined tools
func (t Tool) MarshalJSON() ([]byte, error) {
	// For Anthropic-defined tools (non-function tools), include type and name
	if t.Type != ToolTypeFunction {
		// Determine the appropriate name based on tool type
		// Per Bedrock documentation: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic.html
		name := ""
		switch t.Type {
		case ToolTypeTextEditor20241022, ToolTypeTextEditor20250124:
			name = "str_replace_editor"
		case ToolTypeTextEditor20250728:
			name = "str_replace_based_edit_tool"
		case ToolTypeMemory20250818:
			name = "memory"
		}

		return json.Marshal(struct {
			Type ToolType `json:"type"`
			Name string   `json:"name"`
		}{
			Type: t.Type,
			Name: name,
		})
	}

	// For function tools, include all fields
	type Alias Tool
	return json.Marshal((Alias)(t))
}

type ToolChoice struct {
	Type     ToolType     `json:"type"`
	Function ToolFunction `json:"function,omitempty"`
}

type ToolFunction struct {
	Name string `json:"name"`
}

type FunctionDefinition struct {
	Name        string `json:"name"`
	Description string `json:"description,omitempty"`
	Strict      bool   `json:"strict,omitempty"`
	// Parameters is an object describing the function.
	// You can pass json.RawMessage to describe the schema,
	// or you can pass in a struct which serializes to the proper JSON schema.
	// The jsonschema package is provided for convenience, but you should
	// consider another specialized library if you require more complex schemas.
	Parameters any `json:"parameters"`
}

// Deprecated: use FunctionDefinition instead.
type FunctionDefine = FunctionDefinition

type TopLogProbs struct {
	Token   string  `json:"token"`
	LogProb float64 `json:"logprob"`
	Bytes   []byte  `json:"bytes,omitempty"`
}

// LogProb represents the probability information for a token.
type LogProb struct {
	Token   string  `json:"token"`
	LogProb float64 `json:"logprob"`
	Bytes   []byte  `json:"bytes,omitempty"` // Omitting the field if it is null
	// TopLogProbs is a list of the most likely tokens and their log probability, at this token position.
	// In rare cases, there may be fewer than the number of requested top_logprobs returned.
	TopLogProbs []TopLogProbs `json:"top_logprobs"`
}

// LogProbs is the top-level structure containing the log probability information.
type LogProbs struct {
	// Content is a list of message content tokens with log probability information.
	Content []LogProb `json:"content"`
}

type Prediction struct {
	Content string `json:"content"`
	Type    string `json:"type"`
}

type FinishReason string

const (
	FinishReasonStop          FinishReason = "stop"
	FinishReasonLength        FinishReason = "length"
	FinishReasonFunctionCall  FinishReason = "function_call"
	FinishReasonToolCalls     FinishReason = "tool_calls"
	FinishReasonContentFilter FinishReason = "content_filter"
	FinishReasonNull          FinishReason = "null"
)

func (r FinishReason) MarshalJSON() ([]byte, error) {
	if r == FinishReasonNull || r == "" {
		return []byte("null"), nil
	}
	return []byte(`"` + string(r) + `"`), nil // best effort to not break future API changes
}

type ChatCompletionChoice struct {
	Index   int                   `json:"index"`
	Message ChatCompletionMessage `json:"message"`
	// FinishReason
	// stop: API returned complete message,
	// or a message terminated by one of the stop sequences provided via the stop parameter
	// length: Incomplete model output due to max_tokens parameter or token limit
	// function_call: The model decided to call a function
	// content_filter: Omitted content due to a flag from our content filters
	// null: API response still in progress or incomplete
	FinishReason         FinishReason         `json:"finish_reason"`
	LogProbs             *LogProbs            `json:"logprobs,omitempty"`
	ContentFilterResults ContentFilterResults `json:"content_filter_results,omitempty"`
}

// ChatCompletionResponse represents a response structure for chat completion API.
type ChatCompletionResponse struct {
	ID                  string                 `json:"id"`
	Object              string                 `json:"object"`
	Created             int64                  `json:"created"`
	Model               string                 `json:"model"`
	Choices             []ChatCompletionChoice `json:"choices"`
	Usage               Usage                  `json:"usage"`
	SystemFingerprint   string                 `json:"system_fingerprint"`
	PromptFilterResults []PromptFilterResult   `json:"prompt_filter_results,omitempty"`
	// Citations is a list of URLs or references that the model used in generating its response.
	// This is primarily used by Perplexity AI models.
	Citations []string `json:"citations,omitempty"`

	httpHeader
}

// CreateChatCompletion â€” API call to Create a completion for the chat message.
func (c *Client) CreateChatCompletion(
	ctx context.Context,
	request ChatCompletionRequest,
) (response ChatCompletionResponse, err error) {
	if request.Stream {
		err = ErrChatCompletionStreamNotSupported
		return
	}

	urlSuffix := chatCompletionsSuffix
	if !checkEndpointSupportsModel(urlSuffix, request.Model) {
		err = ErrChatCompletionInvalidModel
		return
	}

	reasoningValidator := NewReasoningValidator()
	if err = reasoningValidator.Validate(request); err != nil {
		return
	}

	// Build request options
	requestOptions := []requestOption{withBody(request)}

	// Add extra headers if provided
	if request.ExtraHeaders != nil && len(request.ExtraHeaders) > 0 {
		requestOptions = append(requestOptions, withHeaders(request.ExtraHeaders))
	}

	req, err := c.newRequest(
		ctx,
		http.MethodPost,
		c.fullURL(urlSuffix, withModel(request.Model)),
		requestOptions...,
	)
	if err != nil {
		return
	}

	err = c.sendRequest(req, &response)
	return
}
